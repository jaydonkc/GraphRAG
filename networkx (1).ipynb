{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "879bab3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a1ba4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b6137c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Any, Optional, Set\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45527f67",
   "metadata": {},
   "source": [
    "vLLM Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85cbbef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class ReasoningStep(BaseModel):\n",
    "    step: str = Field(..., description=\"A reasoning step in the planning process\")\n",
    "    required_info: List[str] = Field(..., description=\"Types of information needed for this step\")\n",
    "\n",
    "class QueryPlan(BaseModel):\n",
    "    reasoning_steps: List[ReasoningStep] = Field(..., description=\"Step-by-step plan to answer the query\")\n",
    "    key_concepts: List[str] = Field(..., description=\"Key concepts that need to be found in the knowledge graph\")\n",
    "    search_strategy: str = Field(..., description=\"Strategy for searching the knowledge graph\")\n",
    "    expected_answer_type: str = Field(..., description=\"What type of answer is expected (causal, descriptive, comparative, etc.)\")\n",
    "\n",
    "class NotebookEntry(BaseModel):\n",
    "    source_node_id: str = Field(..., description=\"ID of the node this information came from\")\n",
    "    information: str = Field(..., description=\"Key information extracted from the node\")\n",
    "    relevance_score: float = Field(..., description=\"How relevant this information is (0-1)\")\n",
    "    information_type: str = Field(..., description=\"Type of information (causal, descriptive, statistical, etc.)\")\n",
    "\n",
    "class ExplorationDecision(BaseModel):\n",
    "    should_continue: bool = Field(..., description=\"Whether to continue exploring\")\n",
    "    reasoning: str = Field(..., description=\"Reasoning for the decision\")\n",
    "    next_nodes_to_explore: List[str] = Field(default=[], description=\"Specific node IDs to explore next\")\n",
    "    exploration_strategy: str = Field(..., description=\"How to explore next (neighbors, keywords, specific_nodes)\")\n",
    "    information_gaps: List[str] = Field(default=[], description=\"What information is still needed\")\n",
    "\n",
    "class FinalAnswer(BaseModel):\n",
    "    reasoning_steps: List[str] = Field(..., description=\"Step-by-step reasoning using gathered information\")\n",
    "    answer: str = Field(..., description=\"Final comprehensive answer to the question\")\n",
    "    confidence: float = Field(..., description=\"Confidence score (0-1) in the answer\")\n",
    "    sources: List[str] = Field(..., description=\"Node IDs used as sources for the answer\")\n",
    "    information_completeness: float = Field(..., description=\"How complete the gathered information is (0-1)\")\n",
    "\n",
    "answerString = \\\n",
    "\"\"\"\n",
    "class Reasoning_Step(BaseModel):\n",
    "    reasoning_step: str = Field(..., description=\"An intermediate reasoning step for breaking down the given context and query\")\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    reasoning: List[Reasoning_Step] = Field(..., description=\"List of reasoning steps\")\n",
    "    conclusion: bool = Field(..., description=\"The culminating final conclusion or answer to the question\")\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b269e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class IterativeKnowledgeGraphAgent:\n",
    "    def __init__(self, gml_file_path: str, vllm_client, tokenizer_name: str = \"mistralai/Mistral-7B-Instruct-v0.3\", max_iterations: int = 5):\n",
    "        \"\"\"\n",
    "        Initialize the iterative Knowledge Graph QA agent\n",
    "        \n",
    "            tokenizer_name: Name of the tokenizer to use\n",
    "            max_iterations: Maximum number of exploration iterations\n",
    "        \"\"\"\n",
    "        self.graph = nx.read_gml(gml_file_path)\n",
    "        self.vllm_client = vllm_client\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.max_iterations = max_iterations\n",
    "        \n",
    "        self.notebook: List[NotebookEntry] = []\n",
    "        self.explored_nodes: Set[str] = set()\n",
    "        self.current_iteration = 0\n",
    "        \n",
    "        self.entities = {}\n",
    "        self.documents = {}\n",
    "        # self.communities = {}\n",
    "        self._index_nodes()\n",
    "        self.query_embedding = None\n",
    "        self.community = self._build_community_index\n",
    "        # self.embedder = SentenceTransformer('all-MiniLM-L6-v2')  \n",
    "\n",
    "    \n",
    "    # def embed(self, text: str):\n",
    "    #     return self.embedder.encode(text)\n",
    "    \n",
    "    def _index_nodes(self):\n",
    "        \"\"\"Index nodes by their types for efficient retrieval\"\"\"\n",
    "        for node_id, node_data in self.graph.nodes(data=True):\n",
    "            labels = node_data.get('labels', [])\n",
    "            \n",
    "            if '__Entity__' in labels or 'Person' in labels:\n",
    "                self.entities[node_id] = node_data\n",
    "            elif 'Document' in labels:\n",
    "                self.documents[node_id] = node_data\n",
    "            # elif '__Community__' in labels:\n",
    "            #     self.communities[node_id] = node_data\n",
    "\n",
    "    def _build_community_index(self) -> Dict[str, List[str]]:\n",
    "        community_map = {}\n",
    "        for source, target, data in self.graph.edges(data=True):\n",
    "            if data.get(\"type\") == \"IN_COMMUNITY\":\n",
    "                community_map.setdefault(target, []).append(source)\n",
    "        return community_map\n",
    "    \n",
    "    def _create_prompt(self, system_message: str, user_message: str, schema: str) -> str:\n",
    "        \"\"\"Create a formatted prompt for the LLM\"\"\"\n",
    "        return self.tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": f\"{system_message}\\n\\nYou MUST adhere to this schema:\\n{schema}\"},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_bos=True,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "    \n",
    "    def reset_agent_state(self):\n",
    "        \"\"\"Reset the agent's state for a new question\"\"\"\n",
    "        self.notebook = []\n",
    "        self.explored_nodes = set()\n",
    "        self.current_iteration = 0\n",
    "        # self.query_embedding = None\n",
    "\n",
    "    \n",
    "    def create_query_plan(self, question: str) -> QueryPlan:\n",
    "        \"\"\"Create a strategic plan for answering the given question\"\"\"\n",
    "        system_message = \"\"\"You are an expert knowledge graph exploration agent. Create a systematic plan \n",
    "        for answering questions using iterative graph exploration. Focus on what information you need to find \n",
    "        and how to search for it effectively.\"\"\"\n",
    "        \n",
    "        user_message = f\"\"\"\n",
    "        Question: {question}\n",
    "        \n",
    "        Create a detailed exploration plan for this question. The knowledge graph contains:\n",
    "        - Entities: Specific concepts, people, conditions, treatments, etc.\n",
    "        - Documents: Research papers and larger text chunks  \n",
    "        - Relationships: CAUSES, IS_ASSOCIATED_WITH, MENTIONS, IN_COMMUNITY\n",
    "        \n",
    "        Your plan should guide iterative exploration to gather comprehensive information.\n",
    "        \"\"\"\n",
    "        \n",
    "        schema = \"\"\"\n",
    "        class ReasoningStep(BaseModel):\n",
    "            step: str = Field(..., description=\"A reasoning step in the planning process\")\n",
    "            required_info: List[str] = Field(..., description=\"Types of information needed for this step\")\n",
    "\n",
    "        class QueryPlan(BaseModel):\n",
    "            reasoning_steps: List[ReasoningStep] = Field(..., description=\"Step-by-step plan to answer the query\")\n",
    "            key_concepts: List[str] = Field(..., description=\"Key concepts that need to be found in the knowledge graph\")\n",
    "            search_strategy: str = Field(..., description=\"Strategy for searching the knowledge graph\")\n",
    "            expected_answer_type: str = Field(..., description=\"What type of answer is expected (causal, descriptive, comparative, etc.)\")\n",
    "        \"\"\"\n",
    "        \n",
    "        prompt = self._create_prompt(system_message, user_message, schema)\n",
    "        \n",
    "        original_schema = self.vllm_client.schema\n",
    "        self.vllm_client.schema = QueryPlan\n",
    "        \n",
    "        result = self.vllm_client(prompt, sampling_params={\n",
    "            \"n\": 1, \"min_tokens\": 100, \"max_tokens\": 800, \"temperature\": 0.1\n",
    "        })\n",
    "        \n",
    "        self.vllm_client.schema = original_schema\n",
    "        return result\n",
    "    \n",
    "    def find_initial_nodes(self, plan: QueryPlan, top_k: int = 10) -> List[str]:\n",
    "        \"\"\"Find initial nodes to start exploration based on the query plan\"\"\"\n",
    "        keywords = plan.key_concepts.copy()\n",
    "        for step in plan.reasoning_steps:\n",
    "            keywords.extend(step.required_info)\n",
    "        \n",
    "        relevant_nodes = []\n",
    "        keywords_lower = [kw.lower() for kw in keywords]\n",
    "        # if self.query_embedding is None:\n",
    "        #     query_text = \" \".join(plan.key_concepts + sum((step.required_info for step in plan.reasoning_steps), []))\n",
    "        #     self.query_embedding = np.array(self.embed(query_text)).reshape(1, -1)\n",
    "        \n",
    "        for node_id, node_data in self.graph.nodes(data=True):\n",
    "            score = 0\n",
    "            searchable_text = \"\"\n",
    "            \n",
    "            for field in ['description', 'text', 'summary', 'full_content']:\n",
    "                if field in node_data:\n",
    "                    searchable_text += \" \" + str(node_data[field])\n",
    "            searchable_text = searchable_text.lower()\n",
    "            \n",
    "            for keyword in keywords_lower:\n",
    "                if keyword in searchable_text:\n",
    "                    score += searchable_text.count(keyword)\n",
    "\n",
    "            # # add cosine similarity if embeddings are available\n",
    "            # if 'embedding' in node_data:\n",
    "            #     embedding = np.array(node_data['embedding']).reshape(1, -1)\n",
    "            #     sim = cosine_similarity(self.query_embedding, embedding)[0][0]\n",
    "            #     score += sim * 5\n",
    "            \n",
    "            if score > 0:\n",
    "                relevant_nodes.append((node_id, score))\n",
    "        \n",
    "        relevant_nodes.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [node_id for node_id, _ in relevant_nodes[:top_k]]\n",
    "    \n",
    "    def extract_information_from_node(self, node_id: str, question: str, plan: QueryPlan) -> Optional[NotebookEntry]:\n",
    "        \"\"\"Extract relevant information from a specific node\"\"\"\n",
    "        if node_id not in self.graph:\n",
    "            return None\n",
    "        \n",
    "        node_data = self.graph.nodes[node_id]\n",
    "        \n",
    "        node_info = f\"Node ID: {node_id}\\n\"\n",
    "        node_info += f\"Labels: {node_data.get('labels', [])}\\n\"\n",
    "        \n",
    "        if 'description' in node_data:\n",
    "            node_info += f\"Description: {node_data['description']}\\n\"\n",
    "        if 'text' in node_data:\n",
    "            node_info += f\"Text: {node_data['text'][:1000]}{'...' if len(str(node_data['text'])) > 1000 else ''}\\n\"\n",
    "        if 'summary' in node_data:\n",
    "            node_info += f\"Summary: {node_data['summary']}\\n\"\n",
    "        if 'full_content' in node_data:\n",
    "            node_info += f\"Full Content: {str(node_data['full_content'])[:500]}{'...' if len(str(node_data.get('full_content', ''))) > 500 else ''}\\n\"\n",
    "        \n",
    "        \n",
    "        neighbors = list(self.graph.neighbors(node_id))\n",
    "        if neighbors:\n",
    "            node_info += f\"Connected to {len(neighbors)} other nodes\\n\"\n",
    "        \n",
    "        system_message = \"\"\"You are an expert information extractor. Extract the most relevant and useful \n",
    "        information from the given node that helps answer the question. Focus on key facts, relationships, \n",
    "        and insights.\"\"\"\n",
    "        \n",
    "        user_message = f\"\"\"\n",
    "        Question: {question}\n",
    "        Query Plan: {plan.model_dump_json(indent=2)}\n",
    "        \n",
    "        Node Information:\n",
    "        {node_info}\n",
    "        \n",
    "        Extract the most relevant information from this node. Determine its relevance score and information type.\n",
    "        \"\"\"\n",
    "        \n",
    "        schema = \"\"\"\n",
    "        class NotebookEntry(BaseModel):\n",
    "            source_node_id: str = Field(..., description=\"ID of the node this information came from\")\n",
    "            information: str = Field(..., description=\"Key information extracted from the node\")\n",
    "            relevance_score: float = Field(..., description=\"How relevant this information is (0-1)\")\n",
    "            information_type: str = Field(..., description=\"Type of information (causal, descriptive, statistical, etc.)\")\n",
    "        \"\"\"\n",
    "        \n",
    "        prompt = self._create_prompt(system_message, user_message, schema)\n",
    "        \n",
    "        original_schema = self.vllm_client.schema\n",
    "        self.vllm_client.schema = NotebookEntry\n",
    "        \n",
    "        try:\n",
    "            result = self.vllm_client(prompt, sampling_params={\n",
    "                \"n\": 1, \"min_tokens\": 50, \"max_tokens\": 400, \"temperature\": 0.1\n",
    "            })\n",
    "            self.vllm_client.schema = original_schema\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting from node {node_id}: {e}\")\n",
    "            self.vllm_client.schema = original_schema\n",
    "            return None\n",
    "    \n",
    "    def decide_next_exploration(self, question: str, plan: QueryPlan) -> ExplorationDecision:\n",
    "        \"\"\"Decide whether to continue exploring and what to explore next\"\"\"\n",
    "        \n",
    "        notebook_summary = \"\\n\".join([\n",
    "            f\"- {entry.information} (relevance: {entry.relevance_score:.2f}, type: {entry.information_type})\"\n",
    "            for entry in self.notebook\n",
    "        ])\n",
    "        \n",
    "        system_message = \"\"\"You are an expert research agent. Based on the information gathered so far, \n",
    "        decide whether you have enough information to answer the question or if you need to explore more. \n",
    "        If exploring more, specify what nodes or areas to focus on next.\"\"\"\n",
    "        \n",
    "        user_message = f\"\"\"\n",
    "        Question: {question}\n",
    "        Query Plan: {plan.model_dump_json(indent=2)}\n",
    "        Current Iteration: {self.current_iteration + 1}/{self.max_iterations}\n",
    "        \n",
    "        Information Gathered So Far:\n",
    "        {notebook_summary if notebook_summary else \"No information gathered yet\"}\n",
    "        \n",
    "        Explored Nodes: {list(self.explored_nodes)}\n",
    "        \n",
    "        Should you continue exploring? If yes, what should you explore next?\n",
    "        \"\"\"\n",
    "        \n",
    "        schema = \"\"\"\n",
    "        class ExplorationDecision(BaseModel):\n",
    "            should_continue: bool = Field(..., description=\"Whether to continue exploring\")\n",
    "            reasoning: str = Field(..., description=\"Reasoning for the decision\")\n",
    "            next_nodes_to_explore: List[str] = Field(default=[], description=\"Specific node IDs to explore next\")\n",
    "            exploration_strategy: str = Field(..., description=\"How to explore next (neighbors, keywords, specific_nodes)\")\n",
    "            information_gaps: List[str] = Field(default=[], description=\"What information is still needed\")\n",
    "        \"\"\"\n",
    "        \n",
    "        prompt = self._create_prompt(system_message, user_message, schema)\n",
    "        \n",
    "        original_schema = self.vllm_client.schema\n",
    "        self.vllm_client.schema = ExplorationDecision\n",
    "        \n",
    "        result = self.vllm_client(prompt, sampling_params={\n",
    "            \"n\": 1, \"min_tokens\": 100, \"max_tokens\": 600, \"temperature\": 0.2\n",
    "        })\n",
    "        \n",
    "        self.vllm_client.schema = original_schema\n",
    "        return result\n",
    "    \n",
    "    #edges arent directional right?\n",
    "    def get_neighbor_nodes(self, node_ids: List[str], max_neighbors: int = 15) -> List[str]:\n",
    "        \"\"\"Get neighboring nodes for further exploration\"\"\"\n",
    "        neighbors = set()\n",
    "        \n",
    "        for node_id in node_ids:\n",
    "            if node_id in self.graph:\n",
    "                node_neighbors = list(self.graph.neighbors(node_id))\n",
    "                neighbors.update(node_neighbors)\n",
    "        neighbors -= self.explored_nodes        \n",
    "        return list(neighbors)[:max_neighbors]\n",
    "    \n",
    "    def generate_final_answer(self, question: str, plan: QueryPlan) -> FinalAnswer:\n",
    "        \"\"\"Generate the final answer using all gathered information\"\"\"\n",
    "        \n",
    "        sorted_entries = sorted(self.notebook, key=lambda x: x.relevance_score, reverse=True)\n",
    "        \n",
    "        notebook_content = \"\"\n",
    "        for i, entry in enumerate(sorted_entries, 1):\n",
    "            notebook_content += f\"{i}. Source: Node {entry.source_node_id}\\n\"\n",
    "            notebook_content += f\"   Information: {entry.information}\\n\"\n",
    "            notebook_content += f\"   Type: {entry.information_type}, Relevance: {entry.relevance_score:.2f}\\n\\n\"\n",
    "        \n",
    "        system_message = \"\"\"You are an expert researcher synthesizing information to provide a comprehensive answer. \n",
    "        Use all the gathered information from your notebook to construct a well-reasoned, complete response.\"\"\"\n",
    "        \n",
    "        user_message = f\"\"\"\n",
    "        Question: {question}\n",
    "        Query Plan: {plan.model_dump_json(indent=2)}\n",
    "        \n",
    "        Information from Knowledge Graph Exploration:\n",
    "        {notebook_content}\n",
    "        \n",
    "        Total iterations completed: {self.current_iteration}\n",
    "        Total nodes explored: {len(self.explored_nodes)}\n",
    "        \n",
    "        Provide a comprehensive answer with clear reasoning steps, confidence assessment, and completeness evaluation.\n",
    "        \"\"\"\n",
    "        \n",
    "        schema = \"\"\"\n",
    "        class FinalAnswer(BaseModel):\n",
    "            reasoning_steps: List[str] = Field(..., description=\"Step-by-step reasoning using gathered information\")\n",
    "            answer: str = Field(..., description=\"Final comprehensive answer to the question\")\n",
    "            confidence: float = Field(..., description=\"Confidence score (0-1) in the answer\")\n",
    "            sources: List[str] = Field(..., description=\"Node IDs used as sources for the answer\")\n",
    "            information_completeness: float = Field(..., description=\"How complete the gathered information is (0-1)\")\n",
    "        \"\"\"\n",
    "        \n",
    "        prompt = self._create_prompt(system_message, user_message, schema)\n",
    "        \n",
    "        original_schema = self.vllm_client.schema\n",
    "        self.vllm_client.schema = FinalAnswer\n",
    "        \n",
    "        result = self.vllm_client(prompt, sampling_params={\n",
    "            \"n\": 1, \"min_tokens\": 300, \"max_tokens\": 2000, \"temperature\": 0.1\n",
    "        })\n",
    "        \n",
    "        self.vllm_client.schema = original_schema\n",
    "        return result\n",
    "    \n",
    "    def answer_question(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Complete iterative pipeline to answer a question using the knowledge graph\n",
    "        \"\"\"\n",
    "        print(f\"Starting iterative exploration for: {question}\")\n",
    "        \n",
    "        self.reset_agent_state()\n",
    "        \n",
    "        # Step 1: Create query plan\n",
    "        print(\"Creating query plan...\")\n",
    "        plan = self.create_query_plan(question)\n",
    "        print(f\"Plan created with {len(plan.reasoning_steps)} steps\")\n",
    "        \n",
    "        # Step 2: Find initial nodes\n",
    "        print(\"  Finding initial nodes...\")\n",
    "        initial_nodes = self.find_initial_nodes(plan, top_k=8)\n",
    "        print(f\"   Found {len(initial_nodes)} initial nodes to explore\")\n",
    "        \n",
    "        exploration_log = []\n",
    "        \n",
    "        # Step 3: Iterative exploration\n",
    "        while self.current_iteration < self.max_iterations:\n",
    "            print(f\"\\nIteration {self.current_iteration + 1}/{self.max_iterations}\")\n",
    "            \n",
    "            # Determine nodes to explore this iteration\n",
    "            if self.current_iteration == 0:\n",
    "                nodes_to_explore = initial_nodes\n",
    "            else:\n",
    "                # Make exploration decision\n",
    "                decision = self.decide_next_exploration(question, plan)\n",
    "                exploration_log.append(decision)\n",
    "                \n",
    "                if not decision.should_continue:\n",
    "                    print(f\"Agent decided to stop: {decision.reasoning}\")\n",
    "                    break\n",
    "                \n",
    "                if decision.next_nodes_to_explore:\n",
    "                    nodes_to_explore = decision.next_nodes_to_explore\n",
    "                elif decision.exploration_strategy == \"neighbors\":\n",
    "                    # Explore neighbors of high-relevance nodes\n",
    "                    high_relevance_nodes = [entry.source_node_id for entry in self.notebook \n",
    "                                          if entry.relevance_score > 0.7]\n",
    "                    nodes_to_explore = self.get_neighbor_nodes(high_relevance_nodes or [entry.source_node_id for entry in self.notebook[-3:]])\n",
    "                else:\n",
    "                    # Find new nodes based on information gaps\n",
    "                    nodes_to_explore = self.find_initial_nodes(plan, top_k=5)\n",
    "            \n",
    "            nodes_to_explore = [n for n in nodes_to_explore if n not in self.explored_nodes]\n",
    "            \n",
    "            if not nodes_to_explore:\n",
    "                print(\"No new nodes to explore\")\n",
    "                break\n",
    "            \n",
    "            print(f\"Exploring {len(nodes_to_explore)} nodes...\")\n",
    "            \n",
    "            # Extract information from nodes\n",
    "            for node_id in nodes_to_explore[:5]:  # Limit to 5 nodes per iter\n",
    "                if node_id not in self.explored_nodes:\n",
    "                    entry = self.extract_information_from_node(node_id, question, plan)\n",
    "                    if entry and entry.relevance_score > 0.3: \n",
    "                        self.notebook.append(entry)\n",
    "                        print(f\"     ✓ Added info from node {node_id} (relevance: {entry.relevance_score:.2f})\")\n",
    "                    \n",
    "                    self.explored_nodes.add(node_id)\n",
    "            \n",
    "            self.current_iteration += 1\n",
    "        \n",
    "        print(\"\\n Generating final answer...\")\n",
    "        final_answer = self.generate_final_answer(question, plan)\n",
    "        \n",
    "        print(f\" Exploration Summary:\")\n",
    "        print(f\"   - Total iterations: {self.current_iteration}\")\n",
    "        print(f\"   - Nodes explored: {len(self.explored_nodes)}\")\n",
    "        print(f\"   - Information gathered: {len(self.notebook)} entries\")\n",
    "        print(f\"   - Final confidence: {final_answer.confidence:.2f}\")\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"plan\": plan,\n",
    "            \"exploration_log\": exploration_log,\n",
    "            \"notebook\": self.notebook,\n",
    "            \"explored_nodes\": list(self.explored_nodes),\n",
    "            \"iterations_completed\": self.current_iteration,\n",
    "            \"final_answer\": final_answer\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec7a975",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23cadda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.graphs.graph_document import GraphDocument\n",
    "from langchain_core.documents import Document\n",
    "from retry import retry\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from prompts import *\n",
    "import llm\n",
    "\n",
    "from vllm_client import VLLMClient\n",
    "\n",
    "# llm_transformer = llm.LLMGraphTransformer(\n",
    "#     model=None,\n",
    "#     prompt=ontology_prompt, # declared up top\n",
    "#     allowed_relationships=[\"CAUSES\", \"IS_ASSOCIATED_WITH\"],\n",
    "#     node_properties=[\"description\"],\n",
    "#     relationship_properties=[\"description\"]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22c7ebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = VLLMClient(schema=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d01e7399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iterative exploration for: What is the causal relationship between insomnia and chronic pain?\n",
      "Creating query plan...\n",
      "Plan created with 0 steps\n",
      "  Finding initial nodes...\n",
      "   Found 8 initial nodes to explore\n",
      "\n",
      "Iteration 1/5\n",
      "Exploring 8 nodes...\n",
      "     ✓ Added info from node 93 (relevance: 0.90)\n",
      "     ✓ Added info from node 335 (relevance: 0.90)\n",
      "     ✓ Added info from node 336 (relevance: 0.90)\n",
      "     ✓ Added info from node 337 (relevance: 0.90)\n",
      "     ✓ Added info from node 338 (relevance: 0.90)\n",
      "\n",
      "Iteration 2/5\n",
      "Agent decided to stop: The gathered information provides a clear causal relationship between insomnia and various pain conditions, such as neck and shoulder pain, back pain, headache, facial pain, stomach and abdominal pain, and hip pain. The information also suggests that insomnia is causally related to the genetic susceptibility of these pain conditions. Therefore, there is no need to explore further as the question has been sufficiently answered.\n",
      "\n",
      "✨ Generating final answer...\n",
      " Exploration Summary:\n",
      "   - Total iterations: 1\n",
      "   - Nodes explored: 5\n",
      "   - Information gathered: 5 entries\n",
      "   - Final confidence: 0.95\n",
      "=== FINAL ANSWER ===\n",
      "Insomnia is causally related to various pain conditions such as neck and shoulder pain, back pain, headache, facial pain, stomach and abdominal pain, hip pain, and multisite pain. The causal relationship is supported by multiple studies, as evidenced by the significant associations found in sources 1, 3, 4, and 5. The studies reveal that insomnia increases the risk of these pain conditions and is causally related to their genetic susceptibility.\n",
      "Confidence: 0.95\n",
      "\n",
      "=== EXPLORATION NOTEBOOK ===\n",
      "Node 93: Sleeplessness/insomnia is significantly associated with the risk of all these pain conditions (Neck and Shoulder Pain, Back Pain, Headache, Facial Pain, Stomach and Abdominal Pain), as well as depression and anxiety. (score: 0.9)\n",
      "Node 335: The study reveals a significant association between insomnia and various pain conditions, including head pain, hip pain, and multisite pain. Insomnia is causally related to the genetic susceptibility of these pain conditions. (score: 0.9)\n",
      "Node 336: The study reveals a significant association between insomnia and various pain conditions, including head pain, hip pain, and multisite pain. Insomnia is causally related to the genetic susceptibility of these pain conditions. (score: 0.9)\n",
      "Node 337: Sleeplessness/insomnia is a significant risk factor for various pain conditions, including back pain, stomach and abdominal pain, facial pain, headache, neck and shoulder pain, and it also causes a reduced predisposition of being pain-free. (score: 0.9)\n",
      "Node 338: Sleeplessness/insomnia is a significant risk factor for various pain conditions, including back pain, stomach and abdominal pain, facial pain, headache, neck and shoulder pain, and it also causes a reduced predisposition of being pain-free. (score: 0.9)\n"
     ]
    }
   ],
   "source": [
    "agent = IterativeKnowledgeGraphAgent(\n",
    "    gml_file_path=\"graph_dump.gml\",\n",
    "    vllm_client=client,\n",
    "    max_iterations=5\n",
    ")\n",
    "\n",
    "question = \"What is the causal relationship between insomnia and chronic pain?\"\n",
    "result = agent.answer_question(question)\n",
    "\n",
    "print(\"=== FINAL ANSWER ===\")\n",
    "print(result[\"final_answer\"].answer)\n",
    "print(f\"Confidence: {result['final_answer'].confidence}\")\n",
    "\n",
    "print(\"\\n=== EXPLORATION NOTEBOOK ===\")\n",
    "for entry in result[\"notebook\"]:\n",
    "    print(f\"Node {entry.source_node_id}: {entry.information} (score: {entry.relevance_score})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79756f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iterative exploration for: What causes chronic pain in patients with depression?\n",
      "Creating query plan...\n",
      "Plan created with 0 steps\n",
      "  Finding initial nodes...\n",
      "   Found 8 initial nodes to explore\n",
      "\n",
      "Iteration 1/5\n",
      "Exploring 8 nodes...\n",
      "     ✓ Added info from node 337 (relevance: 0.90)\n",
      "     ✓ Added info from node 338 (relevance: 0.90)\n",
      "     ✓ Added info from node 93 (relevance: 0.90)\n",
      "     ✓ Added info from node 341 (relevance: 0.90)\n",
      "     ✓ Added info from node 342 (relevance: 0.80)\n",
      "\n",
      "Iteration 2/5\n",
      "Exploring 3 nodes...\n",
      "\n",
      "Iteration 3/5\n",
      "Exploring 3 nodes...\n",
      "\n",
      "Iteration 4/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m kg_qa \u001b[38;5;241m=\u001b[39m IterativeKnowledgeGraphAgent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph_dump.gml\u001b[39m\u001b[38;5;124m\"\u001b[39m, client)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Ask question\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mkg_qa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manswer_question\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat causes chronic pain in patients with depression?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 346\u001b[0m, in \u001b[0;36mIterativeKnowledgeGraphAgent.answer_question\u001b[0;34m(self, question)\u001b[0m\n\u001b[1;32m    343\u001b[0m     nodes_to_explore \u001b[38;5;241m=\u001b[39m initial_nodes\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;66;03m# Make exploration decision\u001b[39;00m\n\u001b[0;32m--> 346\u001b[0m     decision \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecide_next_exploration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m     exploration_log\u001b[38;5;241m.\u001b[39mappend(decision)\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m decision\u001b[38;5;241m.\u001b[39mshould_continue:\n",
      "Cell \u001b[0;32mIn[15], line 250\u001b[0m, in \u001b[0;36mIterativeKnowledgeGraphAgent.decide_next_exploration\u001b[0;34m(self, question, plan)\u001b[0m\n\u001b[1;32m    247\u001b[0m original_schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvllm_client\u001b[38;5;241m.\u001b[39mschema\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvllm_client\u001b[38;5;241m.\u001b[39mschema \u001b[38;5;241m=\u001b[39m ExplorationDecision\n\u001b[0;32m--> 250\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvllm_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmin_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\n\u001b[1;32m    252\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvllm_client\u001b[38;5;241m.\u001b[39mschema \u001b[38;5;241m=\u001b[39m original_schema\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/graphreader-proto/vllm_client.py:67\u001b[0m, in \u001b[0;36mVLLMClient.__call__\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/graphreader-proto/vllm_client.py:45\u001b[0m, in \u001b[0;36mVLLMClient.generate\u001b[0;34m(self, prompt, sampling_params)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema:\n\u001b[1;32m     43\u001b[0m     payload[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mschema\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mschema_json()\n\u001b[0;32m---> 45\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# A common error is if the LLM loops through tokens until it hits the max limit, failing to output proper tokens for the schema.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# Catch that error and regenerate the response using a repetition penalty to prevent looping\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     unwrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap(response, prompt)\n",
      "File \u001b[0;32m/shared/graphrag/lib/python3.10/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/shared/graphrag/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/shared/graphrag/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/shared/graphrag/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/shared/graphrag/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/shared/graphrag/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/shared/graphrag/lib/python3.10/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m/shared/graphrag/lib/python3.10/site-packages/urllib3/connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "# kg_qa = IterativeKnowledgeGraphAgent(\"graph_dump.gml\", client)\n",
    "\n",
    "# Ask question\n",
    "# result = kg_qa.answer_question(\"What causes chronic pain in patients with depression?\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
